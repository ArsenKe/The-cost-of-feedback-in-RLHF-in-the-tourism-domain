# Cost-Effective RLHF for Tourism Chatbots

This repository contains the implementation and research artifacts for my Master's thesis on developing a cost-effective Reinforcement Learning from Human Feedback (RLHF) system for tourism chatbots.

## Abstract
This research addresses the challenge of implementing a cost-effective feedback application for customer service chatbots using RLHF. The study explores strategies to build an RLHF chatbot assistant that relies on human feedback while minimizing implementation costs, specifically tailored for the tourism domain. Using Design Science Research (DSR), the project implements and evaluates various cost-reduction techniques including 8-bit quantization, DPO fine-tuning, and LoRA adaptation.

## Key Contributions
- Development of a robust data collection strategy for human feedback
- Implementation of memory-efficient training techniques (8-bit quantization, gradient checkpointing)
- Comparative analysis of different model architectures (MT5-Large, Llama variants)
- Design of an effective reward mechanism using Direct Preference Optimization (DPO)
- End-to-end RLHF pipeline with Firebase integration for real-time feedback

## Methodology

### Main Research Question
**How to develop a cost-effective and high-performing tourism chatbot using RLHF?**

### Technical Approach
- **Model Architecture**: MT5-Large (1.2B parameters) with LoRA adaptation
- **Training Method**: Direct Preference Optimization (DPO)
- **Efficiency Techniques**:
  - 8-bit quantization (BitsAndBytes)
  - Gradient checkpointing
  - Small batch sizes with gradient accumulation
- **Feedback Infrastructure**:
  - Gradio interface for user interactions
  - Firebase Realtime Database for feedback collection
  - Automated DPO training loop

### Model Training Details
| Parameter                | Value                     |
|--------------------------|---------------------------|
| Base Model               | MT5-Large                 |
| Fine-tuning Method       | DPO + LoRA                |
| Learning Rate            | 1e-6                      |
| Batch Size               | 1 (per device)            |
| Gradient Accumulation    | 16 steps                  |
| Sequence Length          | 256 tokens                |
| Training Samples         | 40,000                    |
| Evaluation Samples       | 10,000                    |

## Implementation

### Core Components
1. **Feedback Collection System**
   - Gradio web interface
   - Firebase Realtime Database integration
   - Quality rating mechanism

2. **DPO Training Pipeline**
   ```python
   # Key components of DPO implementation
   dpo_args = DPOConfig(
       output_dir="./dpo_results",
       per_device_train_batch_size=1,
       gradient_accumulation_steps=4,
       num_train_epochs=6,
       learning_rate=1e-6,
       beta=0.001
   )
   
   trainer = DPOTrainer(
       model=model,
       ref_model=ref_model,
       args=dpo_args,
       train_dataset=tokenized_dataset
   )
3.**Results**

The MT5-Large model achieved consistent training loss reduction:

Training Step	Loss Value
500	10.3892
1000	0.6514
...	...
8000	0.1225

**DPO training showed stable convergence with final loss values around 2.7-2.8.**

**Repository Structure**
├── LLM_finetuning_colab/          # Model training notebooks
├── RLHF_feedback_on_HuggingFaceSpace/  # Feedback interface
├── RLHF_pipeline_automisation/    # Automated training pipeline
├── README.md                      

**Requirements**
Python 3.9+

PyTorch 2.0+

Transformers, TRL, PEFT

Firebase Admin SDK

Weights & Biases (for logging)